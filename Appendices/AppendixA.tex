% Appendix A

\chapter{Experimental Details and Additional Results} % Main appendix title

\label{AppendixA} % For referencing this appendix elsewhere, use \ref{AppendixA}

%----------------------------------------------------------------------------------------

This appendix provides additional experimental details and supplementary results that support the main findings presented in the thesis.

\section{Detailed Experimental Configuration}

\subsection{Hardware Specifications}

The experiments were conducted on Modal.com infrastructure with the following detailed specifications:

\begin{itemize}
\item \textbf{GPU:} NVIDIA A100 80GB SXM4
\item \textbf{Memory:} 1.2TB DDR4 RAM
\item \textbf{CPU:} High-performance multi-core processor
\item \textbf{Storage:} High-speed NVMe SSD
\item \textbf{Network:} High-bandwidth connectivity for data transfer
\end{itemize}

\subsection{Software Environment}

\begin{itemize}
\item \textbf{Operating System:} Ubuntu 20.04 LTS
\item \textbf{Python Version:} 3.11.5 (CPython)
\item \textbf{PyTorch Version:} 2.0.1
\item \textbf{Transformers Version:} 4.30.2
\item \textbf{Sentence-Transformers Version:} 2.2.2
\item \textbf{CUDA Version:} 12.8
\end{itemize}

\section{Complete Training Hyperparameters}

\begin{table}[h]
\centering
\caption{Complete Hyperparameter Configuration}
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
Learning Rate & 2e-5 \\
Warmup Steps & 1,000 \\
Optimizer & AdamW \\
Beta1 & 0.9 \\
Beta2 & 0.999 \\
Weight Decay & 0.01 \\
Gradient Clipping & 1.0 \\
Batch Size & 128 \\
Max Sequence Length & 512 \\
Margin (Triplet Loss) & 0.2 \\
LoRA Rank & 16 \\
LoRA Alpha & 32 \\
LoRA Dropout & 0.1 \\
\bottomrule
\end{tabular}
\end{table}

\section{Statistical Analysis Details}

\subsection{Significance Testing}

All performance comparisons were evaluated using paired t-tests with the following assumptions:

\begin{itemize}
\item \textbf{Null Hypothesis:} No significant difference between model variants
\item \textbf{Alternative Hypothesis:} Significant difference exists
\item \textbf{Alpha Level:} 0.001 (99.9\% confidence)
\item \textbf{Sample Size:} 55,578 query evaluations
\end{itemize}

\subsection{Effect Size Calculations}

Cohen's d was calculated for all pairwise comparisons:

\begin{itemize}
\item Base vs. Full FT (Random): d = 2.41 (large effect)
\item Base vs. Full FT (Hard): d = 2.78 (large effect)
\item Base vs. LoRA FT (Random): d = 2.63 (large effect)
\item Base vs. LoRA FT (Hard): d = 4.21 (very large effect)
\end{itemize}

\section{Additional UMAP Analysis}

\subsection{UMAP Parameters}

The UMAP visualizations used the following parameters:

\begin{itemize}
\item \textbf{n\_neighbors:} 15
\item \textbf{min\_dist:} 0.1
\item \textbf{n\_components:} 2
\item \textbf{metric:} cosine
\item \textbf{random\_state:} 42
\end{itemize}

\subsection{Embedding Space Metrics}

\begin{table}[h]
\centering
\caption{Quantitative Embedding Space Analysis}
\begin{tabular}{lccc}
\toprule
Model & Silhouette Score & Avg Pairwise Distance & Isotropy \\
\midrule
Base SBERT & 0.42 & 0.67 & 0.23 \\
Full FT (Random) & 0.38 & 0.61 & 0.31 \\
Full FT (Hard) & 0.34 & 0.55 & 0.42 \\
LoRA FT (Random) & 0.29 & 0.48 & 0.58 \\
LoRA FT (Hard) & 0.18 & 0.31 & 0.78 \\
\bottomrule
\end{tabular}
\end{table}

\section{Computational Resource Usage}

\subsection{Training Time Analysis}

\begin{table}[h]
\centering
\caption{Detailed Training Resource Usage}
\begin{tabular}{lcccc}
\toprule
Model & GPU Hours & Peak Memory (GB) & Total Epochs & Convergence \\
\midrule
Full FT (Random) & 12.3 & 76.2 & 3 & Smooth \\
Full FT (Hard) & 8.7 & 74.8 & 5 & Stable \\
LoRA FT (Random) & 9.1 & 68.4 & 3 & Gradual \\
LoRA FT (Hard) & 6.2 & 65.9 & 3 & Unstable \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Inference Profiling}

Detailed breakdown of inference time components:

\begin{itemize}
\item \textbf{Model Loading:} 2.3 seconds (excluded from measurements)
\item \textbf{Query Encoding:} 85\% of total inference time
\item \textbf{Similarity Computation:} 12\% of total inference time
\item \textbf{Ranking Operations:} 3\% of total inference time
\end{itemize}

\section{Reproducibility Information}

\subsection{Random Seeds}

All experiments used the following random seeds for reproducibility:

\begin{itemize}
\item \textbf{Python Random Seed:} 42
\item \textbf{NumPy Random Seed:} 42
\item \textbf{PyTorch Random Seed:} 42
\item \textbf{CUDA Random Seed:} 42
\end{itemize}

\subsection{Data Access Information}

\begin{itemize}
\item \textbf{MS MARCO Dataset:} \url{https://microsoft.github.io/msmarco/}
\item \textbf{Custom Hard Negatives:} \url{https://huggingface.co/datasets/manupande21/}
\item \textbf{Fine-tuned Models:} \url{https://huggingface.co/manupande21/}
\item \textbf{Source Code:} \url{https://github.com/omnikingzeno/ms-marco-fine-tuning-experiments}
\end{itemize}

This comprehensive appendix ensures that all experimental details are documented for full reproducibility of the research findings.
