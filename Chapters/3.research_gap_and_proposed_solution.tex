% Chapter 3

\chapter{Research Gap and Proposed Solution} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter3}

%----------------------------------------------------------------------------------------

This chapter identifies the specific research gaps that motivate our investigation and presents our proposed approach to address these gaps. We build upon the literature review to articulate the unique challenges posed by fine-tuning on saturated benchmarks.

\section{Identified Research Gaps}

\subsection{The Saturated Benchmark Problem}

Traditional fine-tuning research assumes that models are being adapted to new or unseen domains. However, modern pre-trained models often undergo extensive domain-specific training during their initial development. Our base model underwent billion-scale pre-training including significant exposure to MS MARCO data, creating a unique research scenario where conventional fine-tuning assumptions may not hold.

This creates a fundamental gap in our understanding: \textit{What happens when we attempt to fine-tune models on benchmarks they have already extensively seen during pre-training?}

\subsection{Scale Disparity in Fine-Tuning}

Current fine-tuning research often focuses on maximizing performance through larger datasets and longer training. However, there is insufficient understanding of the implications when fine-tuning datasets are orders of magnitude smaller than the original pre-training data.

The scale disparity between billion-sample pre-training and million-sample fine-tuning represents a significant methodological challenge that has received limited systematic investigation.

\subsection{Embedding Space Diagnostics}

While performance metrics like MRR and NDCG provide valuable insights into model effectiveness, they offer limited understanding of \textit{why} certain fine-tuning approaches fail. There is a critical need for diagnostic tools that can reveal the underlying geometric changes in embedding spaces during fine-tuning.

\subsection{Parameter-Efficient Methods on Saturated Models}

LoRA and similar parameter-efficient methods have been extensively evaluated on tasks where models are being adapted to new domains. However, their behavior when applied to already domain-optimized models remains poorly understood, particularly in terms of:

\begin{itemize}
\item Computational efficiency during inference
\item Sensitivity to different negative sampling strategies
\item Impact on embedding space geometry
\end{itemize}

\section{Proposed Investigation Framework}

To address these gaps, we propose a comprehensive experimental framework involving five model variants: Base SBERT, Full Fine-Tuning variants (with different negative sampling), and LoRA variants (with matched sampling approaches). This design systematically evaluates both fine-tuning approach and negative sampling strategy impacts.

\subsection{Embedding Space Analysis Methodology}

We use UMAP visualization as our primary diagnostic tool, providing visualization of high-dimensional relationships while preserving local and global structure. This allows direct observation of geometric changes correlating with performance degradation.

\subsection{Computational Efficiency Analysis}

Beyond performance metrics, we analyze inference time measurements, QPS calculations, and training loss convergence patterns to reveal hidden computational costs in parameter-efficient methods.

\section{Research Questions}

Our investigation was motivated by unexpected preliminary findings where fine-tuning approaches consistently underperformed the base model. This counterintuitive result led us to formulate the following research questions:

\subsection{RQ1: Fine-Tuning Effectiveness on Saturated Benchmarks}

\textit{Why do fine-tuning approaches underperform the base model on MS MARCO despite the conventional wisdom that fine-tuning improves performance?}

This question challenges the universal applicability of fine-tuning and investigates the specific conditions under which it may fail.

\subsection{RQ2: Impact of Negative Sampling Strategies}

\textit{How do different negative sampling strategies affect performance when applied to models that have already seen extensive domain data during pre-training?}

This question explores whether sophisticated negative sampling approaches provide benefits or introduce harmful noise in saturated scenarios.

\subsection{RQ3: Geometric Understanding of Performance Degradation}

\textit{What changes occur in embedding space geometry during fine-tuning that correlate with performance degradation?}

This question seeks to provide a geometric explanation for the observed performance failures through embedding space analysis.

\subsection{RQ4: Computational Implications of Parameter-Efficient Methods}

\textit{What are the real-world computational costs of parameter-efficient methods beyond simple parameter counting?}

This question investigates the deployment implications of methods like LoRA in production environments.

\section{Evaluation Framework}

We employ standard information retrieval metrics (MRR at cutoffs 10 and 100, inference time) and introduce diagnostic measures through UMAP embedding space visualization and training dynamics analysis.

\section{Expected Contributions}

This investigation provides: (1) empirical evidence for fine-tuning limitations on saturated benchmarks, (2) diagnostic methodology using embedding space analysis, (3) computational efficiency insights for parameter-efficient methods, and (4) theoretical framework for understanding scale disparities in transfer learning.

\section{Methodological Rigor}

Our approach ensures reliability through controlled experimental conditions, reproducible infrastructure using well-documented cloud computing resources, and comprehensive evaluation of all results.
